{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model uses CNNs to classify audio input into instrument classes.\n",
    "\n",
    "General Procedure:\n",
    "\t- Use STFTs to transform audio input into spectral graphs\n",
    "\t- Use spectral graphs as \"images\" to classify into instrument classes\n",
    "\n",
    "Data Format:\n",
    " \t- Data is stored in tfrecord form obtained from the Nsynth dataset: https://magenta.tensorflow.org/datasets/nsynth\n",
    "\n",
    "Based off of aymericdamien's TensorFlow examples: https://github.com/aymericdamien/TensorFlow-Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops\n",
    "from tensorflow.python import debug as tf_debug\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# For debugging on Windows 10\n",
    "from pyreadline import Readline\n",
    "readline = Readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "LEARNING_RATE = 0.00001\n",
    "NUM_STEPS = 500\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Network Parameters\n",
    "NUM_INPUT = 16384   # spectrogram data input (img shape: 128*128)\n",
    "NUM_CLASSES = 11    # total instrument classes\n",
    "DROPOUT = 0.25      # Dropout, probability to drop a unit\n",
    "\n",
    "# Data paths\n",
    "TRAINING_DATA = \"E:/NSynth/nsynth-train.tfrecord\"\n",
    "TEST_DATA = \"E:/NSynth/nsynth-valid.tfrecord\"\n",
    "EVAL_DATA = \"E:/NSynth/nsynth-test.tfrecord\"\n",
    "MODEL_PATH = \"E:/NSynth/trained_models/\"\n",
    "\n",
    "# Data parameters\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# Test GPU presence\n",
    "# If no errors thrown, GPU is being used\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network\n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['image']\n",
    "\n",
    "        # Spectrogram data input is a 1-D vector of 16384 features (128*128 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 128, 128, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, NUM_CLASSES, DROPOUT, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, NUM_CLASSES, DROPOUT, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "#     loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#         logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    labels = tf.reshape(labels, shape=[-1]) # Squeeze labels from 2D to 1D - don't know why they are fed as 2D\n",
    "    print(labels)\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'E:/NSynth/trained_models/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001C4267037B8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn, model_dir=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "Tensor(\"Reshape:0\", shape=(?,), dtype=int64)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from E:/NSynth/trained_models/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into E:/NSynth/trained_models/model.ckpt.\n",
      "run-start: run #1: 2 fetches; 0 feeds\n",
      "\n",
      "TTTTTT FFFF DDD  BBBB   GGG \n",
      "  TT   F    D  D B   B G    \n",
      "  TT   FFF  D  D BBBB  G  GG\n",
      "  TT   F    D  D B   B G   G\n",
      "  TT   F    DDD  BBBB   GGG \n",
      "\n",
      "TensorFlow version: 1.10.0\n",
      "\n",
      "======================================\n",
      "Session.run() call #1:\n",
      "\n",
      "Fetch(es):\n",
      "  Adam\n",
      "  Mean:0\n",
      "\n",
      "Feed dict:\n",
      "  (Empty)\n",
      "======================================\n",
      "\n",
      "Select one of the following commands to proceed ---->\n",
      "  run:\n",
      "    Execute the run() call with debug tensor-watching\n",
      "  run -n:\n",
      "    Execute the run() call without debug tensor-watching\n",
      "  run -t <T>:\n",
      "    Execute run() calls (T - 1) times without debugging, then execute run() once more with debugging and drop back to the CLI\n",
      "  run -f <filter_name>:\n",
      "    Keep executing run() calls until a dumped tensor passes a given, registered filter (conditional breakpoint mode)\n",
      "    Registered filter(s):\n",
      "        * has_inf_or_nan\n",
      "  invoke_stepper:\n",
      "    Use the node-stepper interface, which allows you to interactively step through nodes involved in the graph run() call and inspect/modify their values\n",
      "\n",
      "For more details, see help..\n",
      "\n",
      "\n",
      "tfdbg> run\n",
      "run-end: run #1: 2 fetches; 0 feeds\n",
      "136 dumped tensor(s):\n",
      "\n",
      "t (ms)        Size (B) Op type                             Tensor name\n",
      "[0.000]       182      Const                               loss/tags:0\n",
      "[0.445]       260      Const                               gradients/ConvNet/dropout/dropout/div_grad/Shape_1:0\n",
      "[1.950]       340      OneShotIterator                     OneShotIterator:0\n",
      "[18.312]      218      Const                               gradients/Mean_grad/Const:0\n",
      "[18.857]      342      VariableV2                          ConvNet/conv2d/bias/Adam:0\n",
      "[18.878]      4.21k    VariableV2                          ConvNet/dense/bias/Adam_1:0\n",
      "[18.901]      3.35k    VariableV2                          ConvNet/conv2d/kernel/Adam:0\n",
      "[18.932]      474      VariableV2                          ConvNet/conv2d_1/bias/Adam:0\n",
      "[18.959]      346      VariableV2                          ConvNet/conv2d/bias/Adam_1:0\n",
      "[19.018]      478      VariableV2                          ConvNet/conv2d_1/bias/Adam_1:0\n",
      "[19.044]      3.35k    VariableV2                          ConvNet/conv2d/kernel/Adam_1:0\n",
      "[25.785]      366      Const                               gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim:0\n",
      "[39.831]      258      Const                               ConvNet/Flatten/flatten/strided_slice/stack_1:0\n",
      "[49.635]      194      Const                               Reshape/shape:0\n",
      "[54.655]      72.23k   VariableV2                          ConvNet/conv2d_1/kernel/Adam:0\n",
      "[54.657]      72.24k   VariableV2                          ConvNet/conv2d_1/kernel/Adam_1:0\n",
      "[54.664]      258      VariableV2                          ConvNet/dense_1/bias/Adam:0\n",
      "[54.669]      262      VariableV2                          ConvNet/dense_1/bias/Adam_1:0\n",
      "[54.678]      44.23k   VariableV2                          ConvNet/dense_1/kernel/Adam_1:0\n",
      "[54.680]      200      Const                               Adam/learning_rate:0\n",
      "[54.692]      3.34k    VariableV2                          ConvNet/conv2d/kernel:0\n",
      "[57.996]      188      Const                               Adam/epsilon:0\n",
      "[60.823]      4.21k    VariableV2                          ConvNet/dense/bias/Adam:0\n",
      "[69.366]      184      Const                               Adam/beta1:0\n",
      "[71.625]      225.00M  VariableV2                          ConvNet/dense/kernel/Adam:0\n",
      "[73.661]      222      Const                               gradients/Mean_grad/Reshape:0\n",
      "[75.839]      44.22k   VariableV2                          ConvNet/dense_1/kernel/Adam:0\n",
      "[78.945]      72.22k   VariableV2                          ConvNet/conv2d_1/kernel:0\n",
      "[80.597]      464      VariableV2                          ConvNet/conv2d_1/bias:0\n",
      "[80.597]      230      Const                               ConvNet/dropout/dropout/keep_prob:0\n",
      "[82.580]      186      VariableV2                          beta1_power:0\n",
      "[83.916]      225.00M  VariableV2                          ConvNet/dense/kernel:0\n",
      "[93.875]      4.20k    VariableV2                          ConvNet/dense/bias:0\n",
      "[98.601]      184      Const                               Adam/beta2:0\n",
      "[99.698]      44.21k   VariableV2                          ConvNet/dense_1/kernel:0\n",
      "[100.807]     72.23k   Identity                            ConvNet/conv2d_1/kernel/read:0\n",
      "[102.150]     225.00M  VariableV2                          ConvNet/dense/kernel/Adam_1:0\n",
      "[103.680]     332      VariableV2                          ConvNet/conv2d/bias:0\n",
      "[107.502]     196      Identity                            beta1_power/read:0\n",
      "[116.645]     4.21k    Identity                            ConvNet/dense/bias/read:0\n",
      "[118.630]     3.35k    Identity                            ConvNet/conv2d/kernel/read:0\n",
      "[121.321]     226      Const                               global_step/Initializer/zeros:0\n",
      "[126.544]     474      Identity                            ConvNet/conv2d_1/bias/read:0\n",
      "[126.770]     186      VariableV2                          beta2_power:0\n",
      "[134.087]     248      VariableV2                          ConvNet/dense_1/bias:0\n",
      "[136.299]     44.22k   Identity                            ConvNet/dense_1/kernel/read:0\n",
      "[137.380]     342      Identity                            ConvNet/conv2d/bias/read:0\n",
      "[140.678]     190      VariableV2                          global_step:0\n",
      "[140.985]     196      Identity                            beta2_power/read:0\n",
      "[148.275]     258      Identity                            ConvNet/dense_1/bias/read:0\n",
      "[153.856]     227      IsVariableInitialized               global_step/IsVariableInitialized:0\n",
      "[163.738]     224      RefSwitch                           global_step/cond/read/Switch:1\n",
      "[163.740]     218      Switch                              global_step/cond/Switch_1:1\n",
      "[177.354]     208      Merge                               global_step/cond/Merge:1\n",
      "[191.286]     212      Merge                               global_step/cond/Merge:0\n",
      "[199.285]     198      Snapshot                            global_step/add:0\n",
      "[3362.853]    225.00M  Identity                            ConvNet/dense/kernel/read:0\n",
      "[173359.801]  712      IteratorGetNext                     IteratorGetNext:1\n",
      "[173361.746]  4.00M    IteratorGetNext                     IteratorGetNext:0\n",
      "[173368.896]  692      Reshape                             Reshape:0\n",
      "[173374.666]  430      Cast                                Cast:0\n",
      "[173434.363]  120.13M  Conv2D                              ConvNet/conv2d/Conv2D:0\n",
      "[174237.278]  120.13M  BiasAdd                             ConvNet/conv2d/BiasAdd:0\n",
      "[175111.695]  120.13M  Relu                                ConvNet/conv2d/Relu:0\n",
      "[175895.481]  30.03M   MaxPool                             ConvNet/max_pooling2d/MaxPool:0\n",
      "[176164.171]  270      ShapeN                              gradients/ConvNet/conv2d_1/Conv2D_grad/ShapeN:1\n",
      "[176171.763]  270      ShapeN                              gradients/ConvNet/conv2d_1/Conv2D_grad/ShapeN:0\n",
      "[176186.335]  56.25M   Conv2D                              ConvNet/conv2d_1/Conv2D:0\n",
      "[176562.377]  56.25M   BiasAdd                             ConvNet/conv2d_1/BiasAdd:0\n",
      "[176937.490]  56.25M   Relu                                ConvNet/conv2d_1/Relu:0\n",
      "[177314.350]  14.06M   MaxPool                             ConvNet/max_pooling2d_1/MaxPool:0\n",
      "[177491.252]  284      Shape                               gradients/ConvNet/Flatten/flatten/Reshape_grad/Shape:0\n",
      "[177499.137]  238      StridedSlice                        ConvNet/Flatten/flatten/strided_slice:0\n",
      "[177506.192]  246      Pack                                ConvNet/Flatten/flatten/Reshape/shape:0\n",
      "[177516.725]  14.06M   Reshape                             ConvNet/Flatten/flatten/Reshape:0\n",
      "[177636.044]  256.21k  MatMul                              ConvNet/dense/MatMul:0\n",
      "[177651.774]  256.21k  BiasAdd                             ConvNet/dense/BiasAdd:0\n",
      "[177660.698]  268      Shape                               gradients/ConvNet/dropout/dropout/div_grad/Shape:0\n",
      "[177667.437]  300      BroadcastGradientArgs               gradients/ConvNet/dropout/dropout/div_grad/BroadcastGradientArgs:1\n",
      "[177667.560]  256.22k  Mul                                 ConvNet/dropout/dropout/div:0\n",
      "[177678.517]  288      BroadcastGradientArgs               gradients/ConvNet/dropout/dropout/div_grad/BroadcastGradientArgs:0\n",
      "[177686.219]  256.27k  RandomUniform                       ConvNet/dropout/dropout/random_uniform/RandomUniform:0\n",
      "[177697.158]  256.25k  Snapshot                            ConvNet/dropout/dropout/random_uniform/mul:0\n",
      "[177708.012]  256.25k  Snapshot                            ConvNet/dropout/dropout/random_uniform:0\n",
      "[177718.165]  256.22k  Add                                 ConvNet/dropout/dropout/add:0\n",
      "[177727.044]  256.23k  Floor                               ConvNet/dropout/dropout/Floor:0\n",
      "[177751.662]  256.22k  Mul                                 ConvNet/dropout/dropout/mul:0\n",
      "[177764.198]  2.96k    MatMul                              ConvNet/dense_1/MatMul:0\n",
      "[177772.528]  2.96k    BiasAdd                             ConvNet/dense_1/BiasAdd:0\n",
      "[177778.162]  564      SparseSoftmaxCrossEntropyWithLogits SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:0\n",
      "[177778.184]  3.05k    SparseSoftmaxCrossEntropyWithLogits SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1\n",
      "[177787.407]  218      Shape                               gradients/Mean_grad/Shape:0\n",
      "[177796.643]  212      Size                                gradients/Mean_grad/Prod:0\n",
      "[177796.780]  172      Mean                                Mean:0\n",
      "[177804.033]  470      Tile                                gradients/Mean_grad/Tile:0\n",
      "[177804.133]  220      Snapshot                            gradients/Mean_grad/floordiv:0\n",
      "[177807.846]  181      ScalarSummary                       loss:0\n",
      "[177811.969]  212      Cast                                gradients/Mean_grad/Cast:0\n",
      "[177819.866]  209      MergeSummary                        Merge/MergeSummary:0\n",
      "[177822.089]  476      RealDiv                             gradients/Mean_grad/truediv:0\n",
      "[177831.041]  620      ExpandDims                          gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims:0\n",
      "[177836.633]  3.09k    Mul                                 gradients/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul:0\n",
      "[177843.121]  44.26k   MatMul                              gradients/ConvNet/dense_1/MatMul_grad/MatMul_1:0\n",
      "[177843.124]  308      BiasAddGrad                         gradients/ConvNet/dense_1/BiasAdd_grad/BiasAddGrad:0\n",
      "[177843.125]  256.26k  MatMul                              gradients/ConvNet/dense_1/MatMul_grad/MatMul:0\n",
      "[177852.494]  292      ApplyAdam                           Adam/update_ConvNet/dense_1/bias/ApplyAdam:0\n",
      "[177857.921]  44.26k   ApplyAdam                           Adam/update_ConvNet/dense_1/kernel/ApplyAdam:0\n",
      "[177858.770]  256.26k  Mul                                 gradients/ConvNet/dropout/dropout/mul_grad/Mul:0\n",
      "[177871.306]  256.27k  Mul                                 gradients/ConvNet/dropout/dropout/div_grad/RealDiv:0\n",
      "[177880.242]  256.26k  Sum                                 gradients/ConvNet/dropout/dropout/div_grad/Sum:0\n",
      "[177888.840]  256.27k  Reshape                             gradients/ConvNet/dropout/dropout/div_grad/Reshape:0\n",
      "[177916.497]  4.26k    BiasAddGrad                         gradients/ConvNet/dense/BiasAdd_grad/BiasAddGrad:0\n",
      "[177935.453]  14.06M   MatMul                              gradients/ConvNet/dense/MatMul_grad/MatMul:0\n",
      "[177935.456]  225.00M  MatMul                              gradients/ConvNet/dense/MatMul_grad/MatMul_1:0\n",
      "[177935.459]  4.24k    ApplyAdam                           Adam/update_ConvNet/dense/bias/ApplyAdam:0\n",
      "[178047.796]  14.06M   Reshape                             gradients/ConvNet/Flatten/flatten/Reshape_grad/Reshape:0\n",
      "[178166.589]  56.25M   MaxPoolGrad                         gradients/ConvNet/max_pooling2d_1/MaxPool_grad/MaxPoolGrad:0\n",
      "[178715.586]  56.25M   ReluGrad                            gradients/ConvNet/conv2d_1/Relu_grad/ReluGrad:0\n",
      "[179300.445]  72.29k   Conv2DBackpropFilter                gradients/ConvNet/conv2d_1/Conv2D_grad/Conv2DBackpropFilter:0\n",
      "[179300.456]  524      BiasAddGrad                         gradients/ConvNet/conv2d_1/BiasAdd_grad/BiasAddGrad:0\n",
      "[179300.456]  30.03M   Conv2DBackpropInput                 gradients/ConvNet/conv2d_1/Conv2D_grad/Conv2DBackpropInput:0\n",
      "[179327.780]  508      ApplyAdam                           Adam/update_ConvNet/conv2d_1/bias/ApplyAdam:0\n",
      "[179336.135]  72.27k   ApplyAdam                           Adam/update_ConvNet/conv2d_1/kernel/ApplyAdam:0\n",
      "[179601.456]  120.13M  MaxPoolGrad                         gradients/ConvNet/max_pooling2d/MaxPool_grad/MaxPoolGrad:0\n",
      "[180424.984]  225.00M  ApplyAdam                           Adam/update_ConvNet/dense/kernel/ApplyAdam:0\n",
      "[180722.369]  120.13M  ReluGrad                            gradients/ConvNet/conv2d/Relu_grad/ReluGrad:0\n",
      "[182043.877]  392      BiasAddGrad                         gradients/ConvNet/conv2d/BiasAdd_grad/BiasAddGrad:0\n",
      "[182043.877]  3.41k    Conv2DBackpropFilter                gradients/ConvNet/conv2d/Conv2D_grad/Conv2DBackpropFilter:0\n",
      "[182070.112]  3.38k    ApplyAdam                           Adam/update_ConvNet/conv2d/kernel/ApplyAdam:0\n",
      "[182076.314]  184      Mul                                 Adam/mul_1:0\n",
      "[182076.318]  180      Mul                                 Adam/mul:0\n",
      "[182076.319]  376      ApplyAdam                           Adam/update_ConvNet/conv2d/bias/ApplyAdam:0\n",
      "[182099.736]  186      Assign                              Adam/Assign:0\n",
      "[182103.021]  188      Const                               Adam/value:0\n",
      "[182103.027]  190      Assign                              Adam/Assign_1:0\n",
      "[182126.395]  176      AssignAdd                           Adam:0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfdbg> pt IteratorGetNext:1\n",
      "Tensor \"IteratorGetNext:1:DebugIdentity\":\n",
      "  dtype: int64\n",
      "  shape: (64, 1)\n",
      "\n",
      "array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [0],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1]], dtype=int64)\n",
      "\n",
      "tfdbg> exit\n",
      "INFO:tensorflow:loss = 12.708687, step = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: user exited from debugger CLI: Calling sys.exit(1).\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\philip~1\\docume~1\\audiotf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Function to parse the TFRecord\n",
    "def _parse_(serialized_example):\n",
    "#     feature_list = {'note': tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64, allow_missing=True),\n",
    "#                 'note_str': tf.FixedLenSequenceFeature(shape=[], dtype=tf.string, allow_missing=True),\n",
    "#                 'instrument': tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64, allow_missing=True),\n",
    "#                 'instrument_str': tf.FixedLenSequenceFeature(shape=[], dtype=tf.string, allow_missing=True),\n",
    "#                 'pitch': tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64, allow_missing=True),\n",
    "#                 'velocity': tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64, allow_missing=True),\n",
    "#                 'sample_rate': tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64, allow_missing=True),\n",
    "#                 'audio': tf.FixedLenSequenceFeature(shape=[1], dtype=tf.float32, allow_missing=True),\n",
    "#                 'qualities': tf.FixedLenSequenceFeature(shape=[1], dtype=tf.int64, allow_missing=True),\n",
    "#                 'qualities_str': tf.FixedLenSequenceFeature(shape=[1], dtype=tf.string, allow_missing=True),\n",
    "#                 'instrument_family': tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64, allow_missing=True),\n",
    "#                 'instrument_family_str': tf.FixedLenSequenceFeature(shape=[], dtype=tf.string, allow_missing=True),\n",
    "#                 'instrument_source': tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64, allow_missing=True),\n",
    "#                 'instrument_source_str': tf.FixedLenSequenceFeature(shape=[], dtype=tf.string, allow_missing=True)}\n",
    "    feature_list = {\n",
    "                'audio': tf.FixedLenSequenceFeature(shape=[1], dtype=tf.float32, allow_missing=True),\n",
    "                'instrument_family': tf.FixedLenSequenceFeature(shape=[], dtype=tf.int64, allow_missing=True),\n",
    "}\n",
    "    \n",
    "    # Extract example by features\n",
    "    example = tf.parse_single_example(serialized_example, feature_list)\n",
    "    \n",
    "    # Convert audio data to normalized spectrogram\n",
    "    spectrogram = audio_ops.audio_spectrogram(example[\"audio\"], window_size=1024, stride=64)\n",
    "    max_val = tf.reduce_max(spectrogram, axis=None)\n",
    "    min_const = tf.constant(255.)\n",
    "    brightness_const = tf.divide(min_const, max_val)   # Calculate normalization constant\n",
    "    brightened_spect = tf.multiply(spectrogram, brightness_const)\n",
    "    minned_spectrogram = tf.minimum(brightened_spect, min_const) # Remove other spikes?\n",
    "    expanded = tf.expand_dims(minned_spectrogram, -1)\n",
    "    resized = tf.image.resize_bilinear(expanded, [128, 128])\n",
    "    squeezed = tf.squeeze(resized, 0)\n",
    "    flipped = tf.image.flip_left_right(squeezed)\n",
    "    normalized_spectrogram = tf.image.transpose_image(flipped)\n",
    "    \n",
    "    # Cast data to input format required by model\n",
    "    image = normalized_spectrogram\n",
    "#     label = tf.cast(example['instrument_family'],tf.int64)\n",
    "    label = example['instrument_family']\n",
    "    return (dict({'image':image}),label)\n",
    "\n",
    "\n",
    "\n",
    "# Define the data input function for training\n",
    "def tfrecord_train_input_fn(batch_size=BATCH_SIZE):\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(TRAINING_DATA).skip(tf.constant(57500, dtype=tf.int64))\n",
    "#     tfrecord_dataset = tf.data.TFRecordDataset(TRAINING_DATA)\n",
    "#     tfrecord_dataset = tfrecord_dataset.shuffle(buffer_size=50000)\n",
    "    tfrecord_dataset = tfrecord_dataset.map(lambda   x:_parse_(x)).shuffle(True).batch(batch_size)\n",
    "    tfrecord_iterator = tfrecord_dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return tfrecord_iterator.get_next()\n",
    "\n",
    "# Train the Model\n",
    "# With debug\n",
    "hooks = [tf_debug.LocalCLIDebugHook(ui_type=\"readline\")]\n",
    "model.train(tfrecord_train_input_fn, steps=NUM_STEPS, hooks=hooks)\n",
    "# without debug\n",
    "# model.train(tfrecord_train_input_fn, steps=NUM_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "# Define the input function for evaluating\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': mnist.test.images}, y=mnist.test.labels,\n",
    "    batch_size=BATCH_SIZE, shuffle=False)\n",
    "# Use the Estimator 'evaluate' method\n",
    "model.evaluate(input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict single images\n",
    "n_images = 4\n",
    "# Get images from test set\n",
    "test_images = mnist.test.images[:n_images]\n",
    "# Prepare the input data\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': test_images}, shuffle=False)\n",
    "# Use the model to predict the images class\n",
    "preds = list(model.predict(input_fn))\n",
    "\n",
    "# Display\n",
    "for i in range(n_images):\n",
    "    plt.imshow(np.reshape(test_images[i], [28, 28]), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Model prediction:\", preds[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
